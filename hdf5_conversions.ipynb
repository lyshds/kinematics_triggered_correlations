{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "#with pd.HDFStore('myfile.h5', 'r') as d:\n",
    "#    df = d.get('TheData')\n",
    "#    df.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-26-b3bbe30498f2>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-b3bbe30498f2>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    def save_pickle(self,data,filename):\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class NetFly(object):\n",
    "    def __init__(self,flynum,rootpath = '/media/imager/FlyDataD/FlyDB/'):\n",
    "        self.flynum = flynum\n",
    "        self.flypath = rootpath + '/Fly%3d'%flynum\n",
    "        self.flatpaths = [os.path.join(dp, f) for dp, dn, fn in os.walk(self.flypath) for f in fn]\n",
    "        try:\n",
    "            self.h5paths = [fn for fn in self.flatpaths if fn[-4:] == 'hdf5']\n",
    "        except(IndexError):\n",
    "            pass\n",
    "        try:\n",
    "            self.txtpaths = [fn for fn in self.flatpaths if fn[-3:] == 'txt']\n",
    "        except(IndexError):\n",
    "            pass\n",
    "        try:\n",
    "            self.pklpaths = [fn for fn in self.flatpaths if fn[-4:] == 'cpkl']\n",
    "        except(IndexError):\n",
    "            pass\n",
    "        try:\n",
    "            self.script_name = [x for x in os.listdir(self.flypath) if '.py' in x][0].split('.py')[0]\n",
    "        except(IndexError):\n",
    "            pass\n",
    "        try:\n",
    "            self.tiffpaths = [fn for fn in self.flatpaths if fn[-4:] == 'tif']\n",
    "        except(IndexError):\n",
    "            pass\n",
    "        try:\n",
    "            self.pngpaths = [fn for fn in self.flatpaths if fn[-3:] == 'png']\n",
    "        except(IndexError):\n",
    "            pass\n",
    "        try:\n",
    "            self.bagpaths = [fn for fn in self.flatpaths if fn[-3:] == 'bag']\n",
    "        except(IndexError):\n",
    "            pass\n",
    "        try:\n",
    "            self.abfpaths = [fn for fn in self.flatpaths if fn[-3:] == 'abf']\n",
    "        except(IndexError):\n",
    "            pass\n",
    "        \n",
    "    def open_signals(self,extensions = ['hdf5','txt','cpkl','tif','png','bag'],verbose = False):\n",
    "        self.h5files = dict()\n",
    "        if not(type(extensions) is list):\n",
    "            extensions = [extensions]\n",
    "        if 'hdf5' in extensions:\n",
    "            if verbose: print('opening hdf5')\n",
    "            for fn in self.h5paths:\n",
    "                key = fn.split('/')[-1].split('.')[0]\n",
    "                self.h5files[key] = h5py.File(fn,'r')\n",
    "                self.__dict__[key] = self.h5files[key][key]\n",
    "        if 'txt' in extensions:\n",
    "            if verbose: print('opening txt')\n",
    "            for fn in self.txtpaths:\n",
    "                key = fn.split('/')[-1].split('.')[0]\n",
    "                with open(fn,'rt') as f:\n",
    "                    self.__dict__[key] = f.readlines()\n",
    "        if 'cpkl' in extensions:\n",
    "            import cPickle\n",
    "            if verbose: print('opening pkl')\n",
    "            for fn in self.pklpaths:\n",
    "                key = fn.split('/')[-1].split('.')[0]\n",
    "                with open(fn,'rb') as f:\n",
    "                    self.__dict__[key] = cPickle.load(f)\n",
    "        if 'tif' in extensions:\n",
    "            from skimage import io\n",
    "            for fn in self.tiffpaths:\n",
    "                key = fn.splsit('/')[-1].split('.')[0]\n",
    "                self.__dict__[key] = io.imread(fn)\n",
    "        if 'png' in extensions:\n",
    "            from skimage import io\n",
    "            for fn in self.pngpaths:\n",
    "                key = fn.split('/')[-1].split('.')[0]\n",
    "                self.__dict__[key] = io.imread(fn)\n",
    "    \n",
    "    def save_pickle(self,data,filename):\n",
    "        import cPickle\n",
    "        if filename.split('.')[-1] == 'cpkl':\n",
    "            with open(os.path.join(self.flypath,filename),'wb') as f:\n",
    "                cPickle.dump(data,f)\n",
    "            self.__dict__[filename.split('.')[0]] = data\n",
    "        else:\n",
    "            with open(os.path.join(self.flypath,filename + '.cpkl'),'wb') as f:\n",
    "                cPickle.dump(data,f)\n",
    "            self.__dict__[filename] = data\n",
    "            \n",
    "    def save_hdf5(self,data,filename,overwrite = False):\n",
    "        import h5py\n",
    "        if filename.split('.')[-1] == 'hdf5':\n",
    "            fn = os.path.join(self.flypath,filename)\n",
    "            if os.path.exists(fn) and overwrite:\n",
    "                os.remove(fn)\n",
    "            h_file = h5py.File(fn)\n",
    "            h_file.create_dataset(filename,data = data,compression = 'gzip')\n",
    "            h_file.flush()\n",
    "            self.__dict__[filename.split('.')[0]] = data\n",
    "        else:\n",
    "            fn = os.path.join(self.flypath,filename + '.hdf5')\n",
    "            if os.path.exists(fn) and overwrite:\n",
    "                os.remove(fn)\n",
    "            h_file = h5py.File(fn)\n",
    "            h_file.create_dataset(filename,data = data,compression = 'gzip')\n",
    "            h_file.flush()\n",
    "            self.__dict__[filename] = data\n",
    "    \n",
    "    def save_txt(self,string,filename):\n",
    "        if filename.split('.')[-1] == 'txt':\n",
    "            fn = os.path.join(self.flypath,filename)\n",
    "            with open(os.path.join(self.flypath,filename),'wb') as f:\n",
    "                f.write(string)\n",
    "            self.__dict__[filename.split('.')[0]] = string\n",
    "        else:\n",
    "            with open(os.path.join(self.flypath,filename + '.txt'),'wb') as f:\n",
    "                f.write(string)\n",
    "            self.__dict__[filename] = string\n",
    "\n",
    "    def save_py(self,string,filename):\n",
    "        if filename.split('.')[-1] == 'py':\n",
    "            fn = os.path.join(self.flypath,filename)\n",
    "            with open(os.path.join(self.flypath,filename),'wb') as f:\n",
    "                f.write(string)\n",
    "            self.__dict__[filename.split('.')[0]] = string\n",
    "        else:\n",
    "            with open(os.path.join(self.flypath,filename + '.py'),'wb') as f:\n",
    "                f.write(string)\n",
    "            self.__dict__[filename] = string\n",
    "\n",
    "    def run_py(self,filename,pass_flynum = False):\n",
    "        import subprocess\n",
    "        if pass_flynum:\n",
    "            return subprocess.Popen(['python',\n",
    "                                     os.path.join(self.flypath,filename),\n",
    "                                     str(self.flynum)])\n",
    "        else:\n",
    "            return subprocess.Popen(['python',os.path.join(self.flypath,filename)])\n",
    "        \n",
    "    def copy_to_flydir(self,filename):\n",
    "        import shutil\n",
    "        shutil.copy(filename,self.flypath)\n",
    "        \n",
    "    def close_signals(self):\n",
    "        for f in self.h5files.values():\n",
    "            f.close()\n",
    "\n",
    "    def get_last_git_comment(self):\n",
    "        \"return a dictionary with the last comments for the repos tracked in the git_SHA.txt file\"\n",
    "        import os\n",
    "        if os.path.join(self.flypath,'git_SHA.txt') in self.txtpaths:\n",
    "            self.open_signals(extensions = ['txt'])\n",
    "            cmnd_strs = [\"git -C '%s' show --no-patch --oneline %s\"%tuple(sh.split(':')) for sh in self.git_SHA]\n",
    "            cmmt_strs = [(r.split(':')[0].split('/')[-1],os.popen(s.strip()).read()) for r,s in zip(self.git_SHA,cmnd_strs)]\n",
    "            cmmt_dict = dict()\n",
    "            for k,v in cmmt_strs:\n",
    "                cmmt_dict[k] = v\n",
    "            return cmmt_dict\n",
    "        else:\n",
    "            import warnings\n",
    "            warnings.warn('Fly%s does not contain a git_SHA.txt file'%(self.flynum))\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: [u'frenet_log', u'kappa_log']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = \"kappa_debug_log.h5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'axis0',\n",
       " u'axis1',\n",
       " u'block0_items',\n",
       " u'block0_values',\n",
       " u'block1_items',\n",
       " u'block1_values']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.unicode_' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8192072e0fc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.unicode_' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "n1[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('kappa_debug_log.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'frenet_log', u'kappa_log']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = hf.get('frenet_log')\n",
    "n2 = hf.get('kappa_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = np.array(n1)\n",
    "n1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2 = np.array(n2)\n",
    "n2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'axis0', u'axis1', u'block0_items', u'block0_values',\n",
       "       u'block1_items', u'block1_values'], \n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hf_1 = h5py.File('frenet.h5', 'w')\n",
    "hf_2 = h5py.File('kappa.h5', 'w')\n",
    "\n",
    "hf_1.create_dataset('dataset_1', data=n1)\n",
    "hf_2.create_dataset('dataset_2', data=n2)\n",
    "#<HDF5 dataset \"dataset_2\": shape (1000, 200), type \"<f8\">\n",
    "#All we need to do now is close the file, which will write all of our work to disk.\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-471fbebbf9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axis0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "n1(['axis0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'axis0', u'axis1', u'block0_items', u'block0_values',\n",
       "       u'block1_items', u'block1_values'], \n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.unicode_"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(n2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "HDFStore requires PyTables, \"No module named 'tables'\" problem importing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mtables\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tables'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-be0193360650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kappa_debug_log.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kappa_log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                 'File %s does not exist' % path_or_buf)\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;31m# can't auto open/close if we are using an iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# so delegate to the iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             raise ImportError('HDFStore requires PyTables, \"{ex}\" problem '\n\u001b[0;32m--> 472\u001b[0;31m                               'importing'.format(ex=str(ex)))\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_complibs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: HDFStore requires PyTables, \"No module named 'tables'\" problem importing"
     ]
    }
   ],
   "source": [
    "reread = pd.read_hdf('kappa_debug_log.h5', 'kappa_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Support for generic buffers has not been implemented.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b85607e34771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/imager/.local/lib/python2.7/site-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         raise NotImplementedError('Support for generic buffers has not been '\n\u001b[0m\u001b[1;32m    354\u001b[0m                                   'implemented.')\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Support for generic buffers has not been implemented."
     ]
    }
   ],
   "source": [
    "read = pd.read_hdf(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
